
I want to create a service in golang. It has two functions: it has a permanent connection to the bluesky jetstream firehose and saves to disk only root posts with tags, keyed by each tag. A root post is a post that's not a reply to another post, so it starts a thread. While a post can have many tags, getting a list of tags is easy as it's stored in its json metadata, so we only need &lt;tag> 1…m &lt;post>. It needs to handle updates and deletions too so it's up to date. it can efficiently handle paginated queries over http of this form: paginator = getRecentRootPostsWithTags(tags=[]tags, quantity=100, paginator=paginator) what's the best tech stack in go for this? In particular some storage engines do pagination very poorly -- they basically run the whole query and discard everything except the specific window of items needed, making pagination very inefficient. I want good unit test and integration test coverage. For integration the #art tag is by far the most common and running for 30s is guaranteed to get at least one post. Is it better to have these as separate services? Or a single service with multiple threads? Seems like quite separate concerns; the http endpoint doesn't even need write access to the storage.

ChatGPT

**Thought about Bluesky firehose service design** for 11 seconds

**Overall Approach**

A solid approach is to implement the ingestion (jetstream reading and storage) and the querying (HTTP endpoint for pagination) with a proper relational database, such as PostgreSQL, or a well-indexed key-value/document store that supports efficient range queries. Given your constraints, PostgreSQL is a strong choice: it’s well-understood, mature, has excellent Go client libraries, can handle efficient keyset pagination, and is easy to test in CI/CD with ephemeral containers.

**Why PostgreSQL?**



1. **Efficient Pagination: \
**PostgreSQL, combined with a proper schema and indexing strategy, enables keyset pagination (a.k.a. seek-based pagination), which is significantly more efficient than offset-based pagination. Instead of offset/limit, you can store a timestamp or an auto-incrementing ID and query posts where `created_at &lt; :lastSeenTime` (or a similar criterion), ordered by `created_at DESC`. This ensures pagination queries do not require scanning large swaths of data unnecessarily.
2. **Flexible Queries with Indexes: \
**For posts keyed by tags, you can have a join table `post_tags(post_id, tag)` with an index on `(tag, created_at)` or `(tag, post_id)`. This way, retrieving recent posts for a set of tags is efficient. If you need multiple tags simultaneously (i.e., posts containing *all* those tags), you can design your queries accordingly (though typically it's more efficient to start simple—like posts matching *any* of those tags—and refine).
3. **Easy Integration and Testing: \
**PostgreSQL integrates well with Go test frameworks. You can use Docker Compose or `testcontainers-go` to spin up a transient database for integration tests. Unit tests can mock the database layer if needed, but integration tests ensure correctness with real queries.

**Golang Libraries and Tools:**



* **Database Access: \
**Use `pgx` (github.com/jackc/pgx) or `database/sql` with `lib/pq`. `pgx` is often recommended for modern Go + Postgres usage due to performance and rich features.
* **Migrations: \
**Use a migration tool like `golang-migrate` (github.com/golang-migrate/migrate) to keep your schema consistent.

**Schema & Indices: \
**Schema might look like: \
sql \
Copy code \
`CREATE TABLE posts (`


```
  id UUID PRIMARY KEY,
  created_at TIMESTAMP WITH TIME ZONE NOT NULL,
  text TEXT NOT NULL,
  -- possibly store normalized tags in a separate table
  -- or store facets/metadata if needed
);

CREATE TABLE post_tags (
  post_id UUID REFERENCES posts(id) ON DELETE CASCADE,
  tag TEXT NOT NULL,
  PRIMARY KEY (post_id, tag)
);

CREATE INDEX idx_post_tags_tag_created_at ON post_tags (tag, post_id); 
-- If 'post_id' corresponds to descending creation, you can join back to posts ordered by created_at.
```


Alternatively, you can add `created_at` to `post_tags` directly if it makes queries simpler: \
sql \
Copy code \
`CREATE TABLE post_tags (`


```
  post_id UUID,
  tag TEXT,
  created_at TIMESTAMP WITH TIME ZONE NOT NULL,
  PRIMARY KEY (post_id, tag)
);

CREATE INDEX idx_post_tags_tag_created_at ON post_tags (tag, created_at DESC);
```


Now you can query: \
sql \
Copy code \
`SELECT p.id, p.text, p.created_at`


```
FROM post_tags pt
JOIN posts p ON p.id = pt.post_id
WHERE pt.tag = ANY($1)
  AND p.created_at < $2
ORDER BY p.created_at DESC
LIMIT $3;

```



* 

**Pagination:**



* Implement a keyset pagination approach. For example, if your last retrieved post had a `created_at` of `2024-12-12T00:44:31Z`, pass that as `lastSeenTime` in the next query. This approach ensures constant-time pagination steps, as you’re using an index to jump directly to the slice of interest.

**Handling Updates and Deletions:**



* Because you have unique IDs and relational integrity, when a post is updated or deleted (as provided by the Jetstream firehose), you can update or remove the relevant rows. Postgres handles this easily and your indices remain efficient.

**Integration Testing:**



* Start a local Postgres container in your integration tests.
* During test setup, run migrations, start reading from Jetstream for ~30s, assert that you have at least one #art-tagged post.
* Run the pagination queries and assert you get the expected results.

**Monolith or Separate Services?**



* The ingestion (firehose reading and storage) and the query interface (HTTP server) could live in a single service as separate goroutines. This simplifies deployment.
* However, logically, these are separate concerns:
    * The “ingestor” reads Jetstream and writes to the database. It never needs to serve HTTP queries.
    * The “query service” serves HTTP requests, reads from the database, and does not need write access (except maybe to handle updates/deletions if you unify them).
* Two separate services (e.g., `ingestor` and `api`) can be beneficial for scaling and fault isolation. The `ingestor` service can run continuously, pulling data from Jetstream into Postgres. The `api` service can scale horizontally for read traffic.
* They can still share the same Postgres instance. Since reads and writes to Postgres are well-defined, separate services is a clean architecture.

**Testing Strategy:**



* **Unit Tests:** Mock out the database interfaces. Test the logic of pagination helpers, JSON parsing, tag extraction, and internal data structures.
* **Integration Tests:** Use a real Postgres instance in Docker. Insert test posts, simulate a small run of the ingestion loop (or just manually insert mock posts), then test the HTTP API endpoints and pagination logic. Verify correctness and performance.

**Conclusion:**



* **Recommended Stack:**
    * Go + Postgres (with pgx).
    * A migration tool (golang-migrate).
    * Well-designed schema with proper indexing.
    * Keyset pagination queries.
    * Two services: one for ingestion (read jetstream, store posts), one for queries (HTTP API), both sharing a Postgres database.
* This stack provides efficient pagination, ease of testing, clear separation of concerns, and robust reliability.

o1


##### **You said:**

what package would I use for jetstream

ChatGPT

Thought about Jetstream integration with Go for 5 seconds

Bluesky’s "jetstream" firehose isn't the same as NATS JetStream—it's just an HTTP endpoint that continuously streams data in a newline-delimited JSON (NDJSON) format. There isn’t currently an official dedicated Go library for connecting to and consuming this Bluesky jetstream feed. Instead, you can handle it using the standard library’s HTTP and JSON capabilities.

**Recommended Approach:**



1. **Use <code>net/http</code> to connect to the firehose endpoint:</strong>
    * Perform a GET request to the Bluesky firehose endpoint (e.g., <code>com.atproto.sync.subscribeRepos</code>).
    * This request will keep the connection open and stream updates line-by-line.
2. <strong>Use <code>bufio.Scanner</code> or a <code>json.Decoder</code> for NDJSON:</strong>
    * NDJSON (Newline Delimited JSON) means each line is a separate JSON object.
    * You can read each line using <code>bufio.Scanner</code> and then parse each line with <code>encoding/json.Unmarshal</code>.
    * Alternatively, you can use a <code>json.Decoder</code> and repeatedly call <code>Decode()</code> if you ensure the stream returns well-formed JSON objects per line.

<strong>Example Sketch:</strong>

go

Copy code


```
resp, err := http.Get("https://bsky.social/xrpc/com.atproto.sync.subscribeRepos")
if err != nil {
    log.Fatal(err)
}
defer resp.Body.Close()

scanner := bufio.NewScanner(resp.Body)
for scanner.Scan() {
    line := scanner.Bytes()
    // Unmarshal the line into your struct
    var event BlueskyEvent
    if err := json.Unmarshal(line, &event); err != nil {
        log.Println("Error unmarshalling line:", err)
        continue
    }

    // Process event...
}
if err := scanner.Err(); err != nil {
    log.Fatal(err)
}
```


**No Need for a Special Package:**



* Since the jetstream is simply a streaming HTTP endpoint, no special package is strictly needed. You rely on:
    * `net/http` for the HTTP request.
    * `bufio` for reading line-by-line.
    * `encoding/json` for JSON parsing.

This approach is straightforward, efficient, and testable. It also avoids external dependencies for just parsing and handling the jetstream data.

If a dedicated Go package emerges in the future specifically for Bluesky’s jetstream or AT Protocol streams, you could consider adopting it. Until then, the standard library is sufficient.

o1


##### **You said:**

but there is a go package here's how I've done it in the past package guzzle import ( 	"context" 	"database/sql" 	"encoding/json" 	"fmt" 	"io" 	"log" 	"log/slog" 	"math" 	"os" 	"path/filepath" 	"sync" 	"time" 	"github.com/bluesky-social/jetstream/pkg/client" 	"github.com/bluesky-social/jetstream/pkg/client/schedulers/sequential" 	"github.com/bluesky-social/jetstream/pkg/models" 	_ "github.com/mattn/go-sqlite3" ) const ( 	maxRetries = 3 	baseRetryDelay = time.Second 	failureTimeout = time.Hour ) var defaultJetstreamURLs = []string{ 	"wss://jetstream1.us-east.bsky.network/subscribe", 	"wss://jetstream2.us-east.bsky.network/subscribe", 	"wss://jetstream3.us-east.bsky.network/subscribe", 	"wss://jetstream4.us-east.bsky.network/subscribe", } // Config holds the guzzle configuration type Config struct { 	DBPath string 	LogPath string 	// Optional custom Jetstream URLs 	JetstreamURLs []string } // Guzzle represents the firehose ingestion service type Guzzle struct { 	config *Config 	db *sql.DB 	client *client.Client 	mu sync.RWMutex 	metrics *metrics 	logger *log.Logger } // metrics tracks operational metrics type metrics struct { 	entriesLogged uint64 	spaceUsed uint64 	lastUpdate time.Time } // New creates a new guzzle instance func New(cfg *Config) (*Guzzle, error) { 	if cfg == nil { 		return nil, fmt.Errorf("config cannot be nil") 	} 	// Set default URLs if none provided 	if len(cfg.JetstreamURLs) == 0 { 		cfg.JetstreamURLs = defaultJetstreamURLs 	} 	// Create database directory if needed 	dbDir := filepath.Dir(cfg.DBPath) 	if err := os.MkdirAll(dbDir, 0755); err != nil { 		return nil, fmt.Errorf("failed to create database directory: %w", err) 	} 	// Open SQLite database 	db, err := sql.Open("sqlite3", cfg.DBPath+"?_journal_mode=WAL&_busy_timeout=5000") 	if err != nil { 		return nil, fmt.Errorf("failed to open database: %w", err) 	} 	// Initialize schema 	if err := initSchema(db); err != nil { 		db.Close() 		return nil, fmt.Errorf("failed to initialize schema: %w", err) 	} 	// Open log file 	logFile, err := os.OpenFile(cfg.LogPath, os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0666) 	if err != nil { 		db.Close() 		return nil, fmt.Errorf("failed to open log file: %w", err) 	} 	g := &Guzzle{ 		config: cfg, 		db: db, 		metrics: &metrics{ 			lastUpdate: time.Now(), 		}, 		logger: log.New(logFile, "", log.LstdFlags), 	} 	// Initialize metrics from existing data 	var count int64 	var totalSize int64 	err = db.QueryRow("SELECT COUNT(*), COALESCE(SUM(LENGTH(data)), 0) FROM posts").Scan(&count, &totalSize) 	if err != nil { 		g.logger.Printf("Warning: Failed to get initial metrics: %v", err) 	} else { 		g.metrics.entriesLogged = uint64(count) 		g.metrics.spaceUsed = uint64(totalSize) 		g.logger.Printf("Initialized metrics from existing data: %d entries, %s", 			count, formatBytes(uint64(totalSize))) 	} 	return g, nil } // initSchema creates the necessary database tables func initSchema(db *sql.DB) error { 	schema := 	CREATE TABLE IF NOT EXISTS posts ( 		id INTEGER PRIMARY KEY AUTOINCREMENT, 		did TEXT NOT NULL, 		post_id TEXT NOT NULL, 		text TEXT NOT NULL, 		created_at DATETIME NOT NULL, 		data JSON, 		UNIQUE(did, post_id) 	); 	CREATE TABLE IF NOT EXISTS connection_logs ( 		id INTEGER PRIMARY KEY AUTOINCREMENT, 		timestamp DATETIME NOT NULL, 		status TEXT NOT NULL, 		url TEXT NOT NULL 	); 	CREATE TABLE IF NOT EXISTS failure_logs ( 		id INTEGER PRIMARY KEY AUTOINCREMENT, 		timestamp DATETIME NOT NULL, 		message TEXT NOT NULL 	); 	CREATE INDEX IF NOT EXISTS idx_posts_created_at ON posts(created_at); 	CREATE INDEX IF NOT EXISTS idx_posts_did_post ON posts(did, post_id); 	` `	_, err := db.Exec(schema) 	return err } // Run runs the guzzle service until the context is cancelled func (g *Guzzle) Run(ctx context.Context, cursor string, cursorDuration string) error { 	g.logger.Println("Starting guzzle service...") 	defer g.logger.Println("Guzzle service stopped") 	// Parse cursor if provided 	var startTime time.Time 	if cursor != "" { 		parsedTime, err := time.Parse("02/01/2006", cursor) // Standard format for DD/MM/YYYY 		if err != nil { 			return fmt.Errorf("invalid cursor format: %w", err) 		} 		startTime = parsedTime 	} else { 		startTime = time.Now() 	} 	// Parse cursor-duration if provided 	duration := time.Hour // default to 1 hour 	if cursorDuration != "" { 		parsedDuration, err := time.ParseDuration(cursorDuration) 		if err != nil { 			return fmt.Errorf("invalid cursor-duration format: %w", err) 		} 		duration = parsedDuration 	} 	// Log start time 	g.logger.Printf("Backfill starting from: %s for duration: %s", startTime.Format(time.RFC3339), duration) 	// Create error channel for goroutine errors 	errCh := make(chan error, 1) 	// Start metrics logger 	metricsCtx, metricsCancel := context.WithCancel(ctx) 	defer metricsCancel() 	go func() { 		if err := g.logMetrics(metricsCtx); err != nil { 			errCh &lt;- fmt.Errorf("metrics logger error: %w", err) 		} 	}() 	// Create a scheduler that will handle events sequentially 	scheduler := sequential.NewScheduler("brandscan_guzzle", slog.Default(), func(ctx context.Context, event *models.Event) error { 		return g.handleEvent(ctx, event) 	}) 	// Create a timer for the cursor-duration 	durationTimer := time.NewTimer(duration) 	defer durationTimer.Stop() 	urlIndex := 0 	for { 		select { 		case &lt;-ctx.Done(): 			g.logger.Println("Received shutdown signal") 			return ctx.Err() 		case &lt;-durationTimer.C: 			g.logger.Println("Cursor-duration elapsed, stopping firehose") 			return nil 		case err := &lt;-errCh: 			g.logger.Printf("Error in background task: %v", err) 			return err 		default: 			url := g.config.JetstreamURLs[urlIndex] 			g.logConnectionStatus(true, url) 			clientConfig := client.DefaultClientConfig() 			clientConfig.WebsocketURL = url 			clientConfig.Compress = true 			var err error 			g.client, err = client.NewClient(clientConfig, slog.Default(), scheduler) 			if err != nil { 				g.logConnectionStatus(false, url) 				return fmt.Errorf("failed to create client: %w", err) 			} 			cursor := startTime.UnixMicro() 			g.logger.Printf("Using cursor: %d", cursor) // Log the cursor value 			err = g.client.ConnectAndRead(ctx, &cursor) 			if err != nil { 				g.logConnectionStatus(false, url) 				if err == context.Canceled { 					return err 				} 				urlIndex = (urlIndex + 1) % len(g.config.JetstreamURLs) 				if urlIndex == 0 { 					g.logAllEndpointsFailed() 					select { 					case &lt;-ctx.Done(): 						return ctx.Err() 					case &lt;-time.After(failureTimeout): 						continue 					} 				} 				backoff := time.Duration(math.Pow(2, float64(urlIndex))) * baseRetryDelay 				select { 				case &lt;-ctx.Done(): 					return ctx.Err() 				case &lt;-time.After(backoff): 					continue 				} 			} 			// Log end time and duration 			endTime := time.Now() 			g.logger.Printf("Backfill completed. Execution time: %s, Time period covered: %s", endTime.Sub(startTime), duration) 		} 	} } // handleEvent processes a single event from the firehose func (g *Guzzle) handleEvent(ctx context.Context, evt *models.Event) error { 	if evt.Kind != models.EventKindCommit || evt.Commit == nil { 		return nil 	} 	if evt.Commit.Collection != "app.bsky.feed.post" || evt.Commit.Operation != models.CommitOperationCreate { 		return nil 	} 	// print the commit record 	g.logger.Printf("Commit: %s", string(evt.Commit.Record)) 	var post struct { 		Text string `json:"text"` 		CreatedAt time.Time `json:"createdAt"` 	} 	if err := json.Unmarshal(evt.Commit.Record, &post); err != nil { 		return fmt.Errorf("failed to decode record: %w", err) 	} 	if post.Text == "" { 		return nil 	} 	// Store in SQLite 	query := 		INSERT INTO posts (did, post_id, text, created_at, data) 		VALUES (?, ?, ?, ?, ?) 		ON CONFLICT (did, post_id) DO NOTHING 	` `	data, err := json.Marshal(evt.Commit.Record) 	if err != nil { 		return fmt.Errorf("failed to marshal post data: %w", err) 	} 	result, err := g.db.ExecContext(ctx, query, 		evt.Did, 		evt.Commit.RKey, 		post.Text, 		post.CreatedAt, 		string(data), 	) 	if err != nil { 		return fmt.Errorf("failed to store post: %w", err) 	} 	if rows, err := result.RowsAffected(); err == nil && rows > 0 { 		g.mu.Lock() 		g.metrics.entriesLogged++ 		g.metrics.spaceUsed += uint64(len(data)) 		g.mu.Unlock() 	} 	return nil } // formatBytes formats bytes into a human readable string with both bytes and MB func formatBytes(bytes uint64) string { 	mb := float64(bytes) / (1024 * 1024) 	return fmt.Sprintf("%d bytes (%.2f MB)", bytes, mb) } // logMetrics logs the current metrics every minute func (g *Guzzle) logMetrics(ctx context.Context) error { 	ticker := time.NewTicker(time.Minute) 	defer ticker.Stop() 	for { 		select { 		case &lt;-ctx.Done(): 			return nil 		case &lt;-ticker.C: 			g.mu.RLock() 			g.logger.Printf("Metrics - Entries: %d, Space Used: %s", 				g.metrics.entriesLogged, 				formatBytes(g.metrics.spaceUsed)) 			g.mu.RUnlock() 		} 	} } // logConnectionStatus logs connection attempts and failures func (g *Guzzle) logConnectionStatus(connected bool, url string) { 	status := "Connected to" 	if !connected { 		status = "Failed to connect to" 	} 	query := 		INSERT INTO connection_logs (timestamp, status, url) 		VALUES (?, ?, ?) 	` `	g.db.Exec(query, time.Now(), status, url) 	g.logger.Printf("Connection status: %s %s", status, url) } // logAllEndpointsFailed logs when all endpoints have failed func (g *Guzzle) logAllEndpointsFailed() { 	query := 		INSERT INTO failure_logs (timestamp, message) 		VALUES (?, ?) 	` `	message := "All endpoints failed, sleeping for 1 hour" 	g.db.Exec(query, time.Now(), message) 	g.logger.Println(message) } // Close closes the guzzle service and cleans up resources func (g *Guzzle) Close() error { 	g.logger.Println("Shutting down guzzle service...") 	g.mu.RLock() 	g.logger.Printf("Final Metrics - Total Entries: %d, Total Space Used: %s", 		g.metrics.entriesLogged, 		formatBytes(g.metrics.spaceUsed)) 	g.mu.RUnlock() 	if err := g.db.Close(); err != nil { 		g.logger.Printf("Error closing database: %v", err) 		return fmt.Errorf("failed to close database: %w", err) 	} 	g.logger.Println("Database closed successfully") 	if closer, ok := g.logger.Writer().(io.Closer); ok { 		if err := closer.Close(); err != nil { 			return fmt.Errorf("failed to close log file: %w", err) 		} 	} 	return nil }

ChatGPT

Thought for 7 seconds

It appears you’re using the `github.com/bluesky-social/jetstream` package, which is part of the Bluesky ecosystem and provides a Go client for connecting to and consuming the Jetstream (firehose) data via a WebSocket interface. This package abstracts away some of the complexity of handling reconnections, cursors, and event scheduling, making it simpler than rolling your own HTTP/NDJSON solution.

**Key parts of this package:**



* **<code>client</code> package: \
</strong>The <code>client.Client</code> type lets you configure connection parameters (e.g., the WebSocket URLs), handle compression, and manage reading events from Jetstream.
* <strong><code>models</code> package: \
</strong>Defines event data structures like <code>models.Event</code> and <code>models.Commit</code>, which represent the AT Protocol events and commits you’ll receive from Jetstream.
* <strong><code>schedulers</code> package: \
</strong>The <code>schedulers</code> subpackage, like <code>schedulers/sequential</code>, provides a scheduling mechanism to handle incoming events in a controlled manner—e.g., ensuring events are processed sequentially to maintain ordering.

In the code you shared, you:



* Use `client.NewClient()` to create a Jetstream client with a given config and scheduler.
* Call `client.ConnectAndRead(ctx, &cursor)` to start receiving events.
* Implement a `handleEvent()` function to process each `models.Event` as it arrives.

This approach leverages the ready-made client logic rather than manually managing an HTTP connection, reading lines, and decoding JSON. It also provides a more stable and maintainable solution if you’re relying on Bluesky’s upstream changes, as the maintainers of `github.com/bluesky-social/jetstream` can keep the library in sync with updates to the protocol or endpoints.

If you’re comfortable with this code and library, sticking with `github.com/bluesky-social/jetstream` is a good idea. It’s a recognized solution for directly interfacing with Bluesky’s Jetstream and will save you from writing a custom streaming and reconnect-handling layer yourself.

o1


##### **You said:**

ok can you walk me through the key set pagination approach that postgres supports

ChatGPT

Thought for a second

**Keyset pagination** (also called **seek-based pagination**) is a technique to page through a dataset more efficiently than traditional offset-based pagination. Instead of using `OFFSET` and `LIMIT` to skip results, you use a known reference point (a key) from the last retrieved record to fetch the next set. This approach avoids the performance overhead associated with large offsets and ensures more stable and consistent pagination, especially as the underlying data changes.

**Why Keyset Pagination?**



* **Performance: \
**With offset-based pagination, the database must scan and discard all rows up to the offset before returning the requested page. For large offsets, this can become expensive.
* **Stability: \
**If new rows are inserted at the start of your dataset (e.g., new posts in a timeline), offset-based pagination might show duplicates or skip results. Keyset pagination uses a stable “anchor” value, so you always fetch “newer than this ID” or “older than this timestamp,” making results stable even as new rows are inserted.
* **Predictability: \
**Keyset pagination typically returns consistent performance from page to page, since it uses index range scans rather than scanning and discarding a large number of rows.

**Example Schema and Query:**

Let’s say we have a `posts` table:

sql

Copy code


```
CREATE TABLE posts (
    id BIGSERIAL PRIMARY KEY,
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    text TEXT NOT NULL,
    -- possibly other fields
);

CREATE INDEX idx_posts_created_at ON posts (created_at DESC);
```


We have an index on `created_at`. We want to page through our posts in descending order of `created_at`—i.e., newest first.

**Traditional Offset Pagination:**

sql

Copy code


```
SELECT id, text, created_at
FROM posts
ORDER BY created_at DESC
LIMIT 100 OFFSET 200;
```


For large offsets, PostgreSQL must scan through (and discard) those first 200 rows before returning the next 100. As offset grows, so does the cost.

**Keyset Pagination Approach:**

**Fetch the first page: \
**For the very first request (no previous page known), just do: \
sql \
Copy code \
`SELECT id, text, created_at`


```
FROM posts
ORDER BY created_at DESC
LIMIT 100;

```



1. Suppose the last record you got had `created_at = '2024-12-12T00:44:31Z'`. You store this value as a bookmark or "cursor" for the next page.

**Subsequent pages: \
**On the next page request, the client sends back the `created_at` of the last record from the previous page. Now we can query: \
sql \
Copy code \
`SELECT id, text, created_at`


```
FROM posts
WHERE created_at < $1    -- $1 is the last retrieved post's timestamp
ORDER BY created_at DESC
LIMIT 100;

```



2. This query finds rows strictly older than the last record we saw. Importantly, because we have an index on `created_at DESC`, the query can efficiently jump into the correct place in the index and return the next 100 rows almost instantly.

**What if we have multiple sort keys or conditions?**

Typically, you want a stable, unique ordering. For example, `created_at` plus `id` if `created_at` is not guaranteed unique: \
sql \
Copy code \
`SELECT id, text, created_at`


```
FROM posts
WHERE (created_at, id) < ($lastCreatedAt, $lastId)
ORDER BY created_at DESC, id DESC
LIMIT 100;

```



* This uses a tuple comparison to ensure a stable ordering. You keep track of both `created_at` and `id` from the last record in the previous page.

**What if we have multiple tags or filtering conditions?**

Keyset pagination still works well as long as you have a stable sorted column. Let’s say you’re filtering by a particular tag: \
sql \
Copy code \
`SELECT p.id, p.text, p.created_at`


```
FROM posts p
JOIN post_tags pt ON p.id = pt.post_id
WHERE pt.tag = $tag
  AND p.created_at < $lastCreatedAt
ORDER BY p.created_at DESC
LIMIT 100;

```



* With a proper composite index on `(tag, created_at)`, PostgreSQL can efficiently jump directly to the correct range for the given tag and `created_at`.

**Storing the Cursor:**



* After retrieving a page, you store the last record’s sort key(s) on the client side (e.g., in a query parameter `?last_created_at=2024-12-12T00:44:31Z`). On the next request, the client sends this back, and the server uses it to produce the next page.
* The server doesn’t need to store session state; the “cursor” is stateless. The client always provides what’s needed.

**Testing and Validation:**



* Test performance with large data sets. Even with millions of rows, keyset pagination remains efficient because each “next page” query uses a selective index range scan.
* Ensure that your application logic handles edge cases:
    * If no more rows match (e.g., you’ve reached the end), return an empty page.
    * If records were inserted after the last page was fetched, the next page will exclude them if the ordering condition strictly enforces `created_at &lt; lastCreatedAt`.

**Summary of Steps for Keyset Pagination:**



1. Choose a stable sort order (commonly by `created_at DESC`).
2. On the first request, query with no constraints and return the first page plus the cursor (e.g., the `created_at` of the last returned record).

On subsequent requests, use the cursor as a condition to fetch rows after that point in the order: \
sql \
Copy code \
`WHERE created_at &lt; $lastCreatedAt`


```
ORDER BY created_at DESC
LIMIT 100

```



3. 
4. Return results and the new cursor again.

**Benefits Over Offset Pagination:**



* Constant time lookups: As pages go deeper, performance doesn’t degrade because you never scan from the start—Postgres uses the index to jump right to the next segment.
* More stable pagination as data changes, minimizing issues with skipping or duplicating rows.

In summary, keyset pagination leverages sorted indexing and a known “last seen” key to fetch the next set of rows efficiently, making it an excellent strategy for large, frequently updated datasets like feed posts.
